<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>MLEF Energy Storage Visuals</title>
    <link rel="stylesheet" href="main.css">
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500&display=swap" rel="stylesheet">
  </head>
  <body>
    <header>
      <h1>
        Energy Storage Topic Modeling <br>
      </h1>
    </header>
    <section class="proj">
      <h2> The Problem </h2>
      <p>
        Storing energy from intermittent renewables, such as wind and solar, is one of the most pressing challenges we face for enabling a sustainable civilization. 
        Scientific research into energy storage technologies has exploded in recent years and sorting through this large body of knowledge to understand the state of field 
        is an important and challenging problem.<br>      
      </p>

      <button type="button" class="collapsible"> Read More </button>
      <div class="extended">
        <p><br> 
          As the need for clean, sustainable energy has become more imperative, the amount of research in the field of energy storage has skyrocketed.<br><br>
        </p>
           <img src="figures/Years.png" alt="Timeline of publications" style='height: 30%; width: 30%; object-fit: contain; margin: 1% 1% 1% 16%'>
           <img src="figures/Frac.png" alt="Timeline of fraction of publications" style='height: 30%; width: 30%; object-fit: contain; margin: 1% 16% 1% 1%'>
        <p><br>
          The plots above respectively show the distribution of publication years for the papers in our dataset 
          and the fraction of papers in the entire Semantic Scholar Open Research Corpus containing the term "Energy Storage" over time.
          They clearly demonstrate that the body of research is rapidly expanding and becoming a more prominent area of focus in the scientific community at large.
          This increase makes it difficult to maintain a high-level understanding of the state of the field 
          and to stay aware of where progress is being made.
          This lack of insight can lead to redundant research, inefficient investing of time and money, and oversight of areas with the greatest potential.
          To optimize effective allocation of resources, investigators and administrative bodies need a broad understanding of the current state of the field including
          different areas of study and their connections as well as trends in areas of interest.
        </p>
      </div>

      <h2> <br> The Goal </h2> 
      <p>
        This project uses a variety of natural language processing techniques to extract insights from the scientific literature. The ultimate goal is to create a tool 
        to better direct research efforts and investments towards promising technologies.<br>
      </p>
      <button type="button" class="collapsible"> Read More </button>
      <div class="extended">
        <p> <br>
          While it's nearly impossible for any individual to read through the tens of thousands of papers found in our dataset, 
          recent AI technology has the power to process all this data and more.  
          Furthermore, the machine learning techniques we use may find patterns and connections between different areas of the field 
          that humans might not be able to recognize on their own.
          In this work, we have generated two interactive plots to allow researchers to explore the models of the field of energy storage.<br><br>

          The first plot shows a graph of topics extracted from the abstracts in our dataset.
          Researchers can click on topics and find related papers and information about topic trends.  
          They can also look at the connections between topics and find papers and trends at the intersection of topics.
          This interactive visualization allows users to easily explore different areas of the field,
          access meta data on papers and topics, and quickly do a literature search in the area of interest
          (More details in the Topic Modeling Section). <br><br>

          The second plot looks at the relationships between individual key words in the data with a technique known as Word2Vec.
          Word2Vec represents each word as a vector that can be visualized with projection onto a 2D plane.
          Researchers can explore similar words in the field and even do word-vector math 
          to look at relationships between different words (details on this in the Word Vector section).
        </p>
      </div>

      <h2> <br> The Data </h2>
      <p>
        The data is composed of ~60,000 abstracts of scientific papers related to energy storage.  
        The papers were pulled from the Semantic Scholar Open Research Corpus database of scientific papers for natural language processing.<br>
      </p>
      
      <button type="button" class="collapsible"> Read More </button>
      <div class="extended">
        <p> <br>
          Our models were trained on 54,125 abstracts from scientific papers relating to energy storage pulled from the Semantic Scholar Open Research Corpus.
          The corpus is a collection of research papers stored in a JSON archive designed for ease of use in natural language processing tasks.
          The corpus is rich with metadata including the fields of study the abstract is from, authors, years published, and the in-citations 
          (papers that cite the current paper) and the out-citations (papers cited by the current paper).  
          The plots below give an overview of these features for our dataset.<br><br>
        </p>
      
      <img src="figures/FOS.png" alt="Fields of Study Figure" class="picture" style='height: 60%; width: 60%; object-fit: contain; margin: 1% 20%'>
      <img src="figures/Length.png" alt="Length of Abstract" style='height: 25%; width: 25%; object-fit: contain; margin: 1% 1% 1% 10%'>
      <img src="figures/InCite.png" alt="Number of external citations" style='height: 25%; width: 25%; object-fit: contain'>
      <img src="figures/OutCite.png" alt="Number of references per paper" style='height: 25%; width: 25%; object-fit: contain; margin: 1% 10% 1% 1%'>
      
        <p> <br>
          To cultivate our dataset, we pulled all abstracts from the entire <a class="outlink" href="https://allenai.org/data/open-research-corpus">semantic scholar dataset</a> 
          with the search term "Energy Storage" in the title or abstract. 
          Then we processed the raw text.  
          First, we excluded papers in any language that wasn't English.  
          Then we removed stop words, which are commonly used words in the English language that carry very little useful information for natural language processing 
          such as (the, a, is, are).
          Since many of the papers in our database are related to chemical and material engineering, we used the
          <a class="outlink" href="https://github.com/materialsintelligence/mat2vec#supplementary-materials-for-unsupervised-word-embeddings-capture-latent-knowledge-from-materials-science-literature-nature--571-9598-2019"> mat2vec</a> processing library
          to standardize the representation of chemical formulas. 
          For example, converting all instances of "Lithium" to "li" to avoid redundancy.
          We used WordNetLemmatizer which converts words to their meaningful base form while also considering the context.
          We also used <a class="outlink" href="https://www.geeksforgeeks.org/introduction-to-stemming/">porter stemming</a> which reduces all variations of words to one root.  
          For example, the words "cycle", "cyclic", "cycled", and "cycling" would all be converted to "cycl".
          This is effective, though may limit interpretability in some cases.  
          The result is the processed text we use to train our models.
        </p>
      </div>
    </section>
    
    <section id="LDA">
      <h2>
        Topic Modeling with LDA 
      </h2>
      <div class="photo">
        <a href="html_files/ES_networkplot.html">
          <img src="figures/lda.PNG", alt="lda" style='height: 100%; width: 100%; object-fit: contain'>
        </a>
      </div>
      <div class="description">
        <p>
          <span class="fieldname"> Description: </span> 
          
          Topic modeling is a machine learning problem where you have a collection (corpus) of documents, and you want to extract topics from the unstructured text data.
          In our case, our corpus is the set of abstracts pulled from the Semantic Scholar Open Research Corpus, and we want to extract topics of interesting research from these texts.
          Once we've extracted these topics, we create a graph where each node is a topic and each edge represents the probability of these topics co-occurring in an abstract.
          Different features of the graph convey further information about how prevalent a topic is or how frequently it has shown up in abstracts in the past five years.<br><br>

          <span class="fieldname"> Goal: </span> 
          
          The goal of this modeling and visualization is to algorithmically produce a high-level overview of the field and 
          to allow users to interactively explore different topics and the connections between them.
          We hope this provides insight into surprising connections between different areas of the field or trending topics of interest.
          This visualization can also be used to explore literature in the field related to each topic or their connections.<br><br>
          
          <span class="fieldname"> Algorithms: </span> 
          
          This visualization relies primarily on two techniques: Latent Dirichlet Allocation (LDA) and Louvain Community Detection.<br>
        </p>
    
        <button type="button" class="collapsible"> Read More </button>
        <div class="extended">
          <p><br>
            <span class="bolded"> Latent Dirichlet Allocation (LDA) </span> is a popular method used for topic modeling.
            It models topics as probability distributions over words. <br> 
            (<span class="emp"> ex: the topic of "pets" has a high probability of containing the word "cat" and a low probability of containing the word "solar_panel". </span>)<br>
            Topics can be represented by the words that have the highest probability of belonging to the topic. 
            Furthermore, each document has a certain probability of containing each topic.
            (<span class="emp"> ex: An article about healthy pet diets has a high probability of containing the topics "food", "pets", and "health" and a low probability of containing the topic "energy_storage."</span>)<br><br>
          </p>

          <a href="https://thinkinfi.com/latent-dirichlet-allocation-for-beginners-a-high-level-overview/">
            <img src="figures/lda_fig.png", alt="lda figure" style='height: 50%; width: 50%; object-fit: contain; margin: 1% 24%'>
          </a>      

          <p><br><br>
            We performed topic modeling with 50 topics and then produced a graph where nodes represent topics and edges represent the probability of topics occuring in the same document
            (<span class="emp"> ex: the topics "health" and "food" are more likely to co-occur than "pets" and "energy_storage".</span>) <br>
          </p>
          
          <button type="button" class="collapsible"> Read More </button>
          <div class="extended" id="lda">
            <p> <br>
              Latent Dirichlet Allocation is a generative probabilistic model often used for topic modeling.  It was first applied to machine learning 
              by David Blei, Andrew Ng, and Michael Jordan in their 2003 paper <a class="outlink" href="https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf"> Latent Dirichlet Allocation </a>.
              A generative probabilistic model simulates the random production of some outcome by defining a graph in which the nodes are conditionally 
              dependent random variables and the edges represent their relationship.<br><br>

              For instance, if we wanted to simulate whether or not you wear a coat each day for a week, we might model it using the graph below.<br>
            </p>

            <img src="figures/gen_prob_model.PNG" alt="generative probabilistic model" style='height: 50%; width: 50%; object-fit: contain; margin: 1% 24%'>

            <p> <br>
              Each day, there's some probability of it being cloudy or sunny.  Depending on that, there's some probability of it being cold or warm.
              And depending on that, there's some probability of you wearing a coat.  
              If we know the exact probability distributions, we can generate a simulation of your coat wearing habits.
              However, it's often difficult to know all of the probability distributions.
              Often, the only observable part of the graph is the outcome (in this example, whether or not you're wearing a coat).
              In this case, we can apply sampling methods and <a class="outlink" href="http://mathcenter.oxford.emory.edu/site/math117/bayesTheorem/"> Bayes theorem </a> 
              to use the observable variables to estimate the underlying distributions for the latent variables (unobservable variables).<br><br>
              
              Using a more complex generative model, we can generate a simplified simulation of writing scientific papers.
              The model used in LDA is a three tier hierarchical Bayesian model. 
              The boxes are "plates" representing repeated entities such as multiple papers, words, and topics.<br><br>
            </p>

            <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">
              <img src="figures/LDA_plate.png" alt="LDA plate model" style='height: 50%; width: 50%; object-fit: contain; margin: 1% 24%'>
            </a>

            <p><br>
              The only observable variable is the words in each paper.  The latent variables are the topics in the corpus of papers.
              In this model, each topic is a <a class="outlink" href="http://mathcenter.oxford.emory.edu/site/math117/multinomialDistribution"> multinomial distribution</a> 
              over words with parameter &phi; representing the probability of each word being used in that topic.
              Each paper is a multinomial distribution over topics with parameter &theta; representing the probability of each topic being used in that document.
              So for each document <span class="bolded">M</span> in our corpus, 
              there is a probability of it including topic <span class="bolded">Z</span> 
              which in turn has a probability of including each word <span class="bolded">N</span> in the document.<br><br>
              
              Taking it one step further, we model the parameters of our multinomial distributions 
              using <a class="outlink" href="https://towardsdatascience.com/dirichlet-distribution-a82ab942a879"> Dirichlet distributions</a>.
              In order to get the parameters &phi;<span class="sub">1</span>...&phi;<span class="sub">K</span> 
              representing the probability of each word being used in topics 1-<span class="bolded">K</span>, 
              we use a Dirichlet distribution with parameter &beta;.
              We use another Dirichlet distribution with parameter &alpha; to get the parameters &theta;<span class="sub">1</span>...&theta;<span class="sub">M</span> 
              representing the probability of each topic being used in documents 1-<span class="bolded">M</span>.
              The Dirichlet distributions is the <a class="outlink" href="https://towardsdatascience.com/conjugate-prior-explained-75957dc80bfb"> conjugate prior </a> 
              for the multinomial distribution parameters.  For our model, we set &alpha; as 1/num_topics and &beta; as 0.03 where num_topics=50.<br><br>

              Now that we have our model, we can reverse engineer it using the words to infer the topics.
              To do this, we use <a class="outlink" href="https://towardsdatascience.com/gibbs-sampling-8e4844560ae5"> Gibbs sampling </a> 
              which is a Markov chain Monte Carlo algorithm for approximating multivariate probability distributions.
              This process approximates the joint distribution for all random variables in our plot.
              At the end of this process, we can define each topic by the top most probable words for that topic.
              We can also keep track of the documents with the highest probability of containing each topic 
              as well as the probability of two topics co-occurring in the same document.
              This information can be used to define a graph in which the nodes are topics and the edges are weighted probabilities of topics co-occurring.

              This work was largely inspired and based off of the work of Manuel W. Bickel in 2019 
              <a class="outlink" href="https://energsustainsoc.biomedcentral.com/articles/10.1186/s13705-019-0226-z">Reflecting trends in the academic landscape of sustainable energy using probabilistic topic modeling</a>.
            </p>
          </div>

          <p> <br><br> 
            <span class="bolded"> Louvain Community Detection </span>
            The Louvain method for community detection is used on large intereconnected graphs to separate nodes into related groups or communities.<br>
          </p>
          
          <a href="https://towardsdatascience.com/community-detection-algorithms-9bd8951e7dae">
            <img src="figures/louvain_fig.png", alt="louvain figure" style='height: 50%; width: 50%; object-fit: contain; margin: 1% 24%'>
          </a>
         
          <p><br><br>
            It starts out by defining each node as an individual community.
            It then uses a similarity metric to compare adjacent nodes, and if they are similar enough, it merges the two communities.
            When it has gone through every node, it creates a new graph in which each community is a single node.
            It repeats this process until no nodes are similar enough to merge.
            The end result is a grouping of similar nodes into research communities.
            These are represented by the colors of the nodes in our graph.<br>
          </p>
          
          <button type="button" class="collapsible"> Read More </button>
          <div class="extended" id="louvain">
            <p><br> 
              The Louvain method for community detection seeks to extract communities from large networks.
              The method was invented by Blondel et al. in their 2008 paper <a class="outlink" href="https://arxiv.org/pdf/0803.0476.pdf"> Fast unfolding communities in large networks</a>. 
              The communities are sets of highly interconnected nodes such that connections within communities are dense while connections between communities are sparse.<br><br>
              
              The algorithm has two iterative phases.
              To begin, each node in the network is assigned a different community.<br><br>

              <span class="bolded">Phase 1:</span>
              For each node, <span class="bolded">i</span>, the gain modularity is calculated for the event in which <span class="bolded">i</span> 
              is removed from its community and placed in the community of each of its neighbors <span class="bolded">j</span>.
              The modularity is a value in the range of [-1/2,1] measuring the ratio of edge density inside vs between communities defined as
            </p>

            <img src="figures/modularity.png" alt="modularity formula" style='height: 30%; width: 30%; object-fit: contain; margin: 1% 34%'>
            
            <p> <br>
              Where<br>
              <span class="bolded">m</span> is the total sum of edge weights in the graph<br>
              <span class="bolded">A</span><span class="sub">ij</span> is the edge weight between nodes <span class="bolded">i</span> and <span class="bolded">j</span><br>
              <span class="bolded">k</span><span class="sub">i</span> and <span class="bolded">k</span><span class="sub">j</span> are the sums of the weights of the edges 
              connected to nodes <span class="bolded">i</span> and <span class="bolded">j</span> respectively.<br>
              <span class="bolded">&delta;</span> is the Knonecker delta function where <span class="bolded">&delta;</span>(x,y)=1 if x=y and 0 otherwise.<br>
              <span class="bolded">c</span><span class="sub">i</span> and <span class="bolded">c</span><span class="sub">j</span> are the communities of the 
              nodes <span class="bolded">i</span> and <span class="bolded">j</span> respectively.<br><br>

              The modularity gain is the change in modularity defined by<br>
            </p>

            <img src="figures/modularity_gain.png" alt="modularity gain formula" style='height: 60%; width: 60%; object-fit: contain; margin: 1% 19%'>

            <p><br>
              Where <br>
              <span class="bolded">&Sigma;</span><span class="sub">in</span> is the sum of the weights of the edges inside the community <span class="bolded">C</span> that <span class="bolded">i</span> is joining<br>
              <span class="bolded">k</span><span class="sub">i,in</span> is the sum of the weights of edges from <span class="bolded">i</span> to nodes in <span class="bolded">C</span><br>
              <span class="bolded">m</span> is the total sum of edge weights in the graph<br>
              <span class="bolded">&Sigma;</span><span class="sub">tot</span> is the sum of the weights of the edges incident to nodes in the <span class="bolded">C</span><br>
              <span class="bolded">k</span><span class="sub">i</span> is the sum of the weights of edges incident to node <span class="bolded">i</span><br><br>
            
              If the max gain is positive, then the node <span class="bolded">i</span> is placed in the community with maximum gain.
              Otherwise <span class="bolded">i</span> remains in its own community.<br><br>

              <span class="bolded">Phase 2:</span> 
              A new network is defined in which the nodes are the communities found during the previous phase.
              The new edge weights are set to the sum of the weights of the edges between the two communities in the previous network.
              Edges between nodes in the same community become self-loops in the new network.<br><br>
            </p>

            <a href="https://www.nature.com/articles/s41598-019-41695-z">
              <img src="figures/louvain_fig2.png", alt="louvain figure 2" style='height: 50%; width: 50%; object-fit: contain; margin: 1% 24%'>
            </a>
            
            <p><br><br>
              Once the second phase is complete, the first phase is repeated on this new graph, and the process repeats until there is no positive modularity gain from combining communities.<br><br>
            </p>
          </div>
         </div> 
        
         <!---LEE, the results for LDA are here-->
        <p><br>
          <span class="fieldname">Results: </span>
          We found that the model generated topics that were sensible and specific, and that the community detection made groupings that were interpretable and interesting.
          Some topics were technology-focused such as topic 49 which is centered around electric vehicles.  Other concepts were more abstract such as topic 23 which had more
          to do with grid optimization and energy management.  Below you can find details of specific results and observations.<br>
        </p>
        
        <button type="button" class="collapsible"> Read More </button>
        <div class="extended">
          <p><br>
            While there is still much exploration and discovery to do, we've compiled some of the observations we've made from using this visualization.
            We found an abundance of interesting and useful categories, some of which are listed below<br><br>

            <span class="bolded">Topic 3:</span> lithium ion batteries and novel chemistries for electrodes<br>
            <span class="bolded">Topic 50:</span> research involving finding substitutes for lithium in battery cathodes<br>
            <span class="bolded">Topic 33:</span> nano-materials in super capacitors and batteries<br>
            <span class="bolded">Topic 41:</span> catalysts for splitting water (oxygen evolution reaction)<br>
            <span class="bolded">Topic 37:</span> fuel cells<br>
            <span class="bolded">Topic 23:</span> grid optimization<br>
            <span class="bolded">Topic 49:</span> electric vehicles.<br><br>

            Note, these topic descriptions were generated by us from the topic keywords and the related papers.
            For instance, Topic 49 has the keywords "vehicl, electr, ev, drive, hybrid, brake" which we then summarize as "electric vehicles".<br><br>

            The connections between these nodes are also of great interest.<br><br>
          </p>
        
          <img src="figures/37-41-fig.png", alt="topic 41 and topic 37 connection" style='height: 50%; width: 50%; object-fit: contain; margin: 1% 24%'>
          
          <p><br>
            For instance, at the intersection of Topic 41 and Topic 37, we find papers that relate to testing of fuel cells with a focus on the electrochemical components
            This intersection makes sense because topic 41 relates to water splitting catalysis, and topic 37 is relates to general fuel cell design and testing.
          <br><br>
          </p>
          
          <img src="figures/23-49-fig.png", alt="topic 23 and topic 49 connection" style='height: 50%; width: 50%; object-fit: contain; margin: 1% 24%'>

          <p><br><br>
            The intersection between Topic 23 and Topic 49 is also of great interest.
            Topic 23 is a more abstract topic we've labeled "Grid Optimization".
            It involves energy storage through regulating grid use through things like virtual energy storage and demand side management.
            For instance, using the internet of things, your dryer could wait to run until demand was low and the cost of electricity is low.
            Topic 49 is generally about electric vehicles.
            The intersection between Topic 23 and Topic 49 is about vehicle to grid and grid to vehicle interactions such as creating optimal charging strategies
            or using electric vehicles as power sources while they're plugged into the grid.<br><br>

            These are just two examples of what our model is capable of.  There are many more topics and connections to explore.<br><br>

            On a broader scale, the communities also are of interest.
            While people with different expertise could pull out more nuanced descriptions of each category, and the categories are much more diverse than these descriptions, 
            it seems that broadly, <br>
            <span class="bolded">Group 0:</span> Thermal energy storage systems<br>
            <span class="bolded">Group 1:</span> Abstract concepts such as research, economics, optimization, and management<br>
            <span class="bolded">Group 2:</span> Battery and capacitor chemistry<br>
            <span class="bolded">Group 3:</span> Energy in biological systems<br>
            <span class="boled">Group 4:</span> Technologies utilizing motors/generators (EVs, Flywheels) <br>
            <span class="bolded">Group 5:</span> Wind and solar as well as energy control, load, and management<br>
            <span class="bolded">Group 6:</span> Nano-materials for batteries and supercapacitors<br><br>

            There's a great diversity within each community, and further investigation can provide deeper insight into what actually connects the communities.
            One example of this is group 4.
            This group includes topic 17 with keywords "flywheel, rotor, speed, rotat, machin, bear" and topic 49 with keywords "vehicl, electr, ev, drive, hybrid, brake".
            We believe this grouping exists because regenerative breaking in electric vehicles uses the same concept as flywheel energy storage.
            Examining these topic groupings has the potential to provide insight that looking at each individual topic or connections between only two topics would not.
          
          </p>
        </div>

        <p> <br>
          <span class="fieldname"> Visuals: </span> 
          <a class="btn" href="html_files/ES_networkplot.html"> Energy Storage Network Plot </a>
        </p>
      </div>
        
    </section>
    <section id="w2v">
      <h2>
        Word Vector Visualization
      </h2>
      <div class="photo">
        <a href="html_files/W2V_tSNE.html">
          <img src="figures/w2v.png", alt="w2v" class="picture" style='height: 100%; width: 100%; object-fit: contain'>
        </a>
      </div>
      <div class="description">
        <p>
          <span class="fieldname"> Description: </span> 
          In natural language processing, words are often embedded into numerical vectors that encode the meanings of the words.
          The embeddings are learned from the context of surrounding words using machine learning algorithms.
          We can easily compare how similar words are by comparing how close their word vectors are.
          Another useful feature of word vectors is that you can add and subtract word vectors to search for relevant words.
          We've plotted some of the top word vectors on a 2D plane, and provided a tool for users to do vector math with words in our corpus.<br><br>
        </p>
        <p>
          <span class="fieldname"> Goal: </span> 
          The goal of this visualization is to be able to choose words out of the collection of abstracts and explore their relationships either by
          seeing how close they are on the two-dimensional visualization, or by experimenting with vector math. <br><br>
        </p>
        
        <p>
          <span class="fieldname"> Algorithms: </span> 
          This visualization relies primarily on two techniques: Word2Vec and t-distributed Stochastic Neighbor Embedding (tSNE).<br>
        </p>

        <button type="button" class="collapsible"> Read More </button>
        <div class="extended">
          <p><br>
            <span class="bolded"> Word2Vec </span>
            Word2vec is a word embedding technique that uses a neural network to learn word associations from a large corpus of text.
            For each word, it uses the current word embedding to try to predict the words in some window around it.
            When it incorrectly predicts a word, it uses the error to update the word embedding and improve.
            The end result is a word embedding for each word in the corpus that is learned from the context of surrounding words.<br><br>          
          </p>

          <a href="https://jalammar.github.io/illustrated-word2vec/">
            <img src="figures/king_man.png", alt="word math fig" style='height: 50%; width: 50%; object-fit: contain; margin: 1% 24%'>
          </a>

          <p><br><br>
            One interesting thing you can do with word vectors is to add and subtract them to find related words.
            A classic example is that if you take the embedding of <span class="bolded">"King"</span> and subtract the embedding of <span class="bolded">"Man"</span>, 
            the resultant vector represents the relationship between <span class="bolded">"King"</span> and <span class="bolded">"Man"</span>.
            If you then add this new vector to the embedding of <span class="bolded">"Woman"</span>, 
            the resulting vector is most similar to the embedding of <span class="bolded">"Queen"</span>.
          </p>
          
          <button type="button" class="collapsible"> Read More </button>
          <div class="extended" id="word2vec">
            <p> <br>
              When working with non-numeric data in machine learning, it is often necessary to create a numeric representation of the data in order to perform operations and analysis.
              One way of doing this is to embed the feature in a numeric vector in which each place in the vector represents a different aspect of the feature.
              For instance, we often represent color as a vector of length 3 in which the first value represents the intensity of red light,
              the second value represents the intensity of green light, and the third value represents the intensity of blue light, all on a scale of 0 to 255.<br><br>
            </p> 
            
            <img src="figures/colorvec.PNG" alt="Color Vectors" style='height: 40%; width: 40%; object-fit: contain; margin: 1% 29%'>
            
            <p><br>
              When creating word embeddings for use in natural language processing tasks, the goal is to create a vector representation of each word in our vocabulary.
              However, unlike with color, we don't have a good idea of what each of the values should represent about the word.
              We could attempt to construct some metric by hand, maybe having different values represent parts of speech, plurality, or length, but coming up with an 
              accurate model this way would be nearly impossible.
              Instead, we use machine learning algorithms to learn embeddings from the natural patterns of words in written texts.<br><br>

              Word2Vec is one such algorithm proposed by Google in 2013 in the paper <a class="outlink" href="https://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</a>.
              One of the main ideas of Word2Vec is that the vectors it produces preserve the similarities of word representations.
              That is to say, similar words in our vocabulary (for example "boy" and "man") will have a greater <a class="outlink" href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a> and will be closer together in vector space.
              One interesting result of this is that you can perform operations using the vectors and find the nearest word vector to the solution.
              As described above, the most famous example of this is that you can take the word vector for "king" and subtract the word vector for "man".
              The result will be a new word vector representing the difference between "king" and "man".
              If you then add this result to the vector for "woman", you get a new word vector, that is most similar to the word vector for "queen".
              Ultimately, by summing negative and positive word representations, you can find similar words derived from the meanings of the input words.<br><br>
            </p>
            
            <a href="https://jalammar.github.io/illustrated-word2vec/">
              <img src="figures/king_queen.png", alt="word math fig" style='height: 50%; width: 50%; object-fit: contain; margin: 1% 24%'>
            </a>
            
            <p><br><br>
              To learn these useful word embeddings, Word2Vec uses a <a class="outlink" href="https://purnasaigudikandula.medium.com/a-beginner-intro-to-neural-networks-543267bda3c8">neural network</a>. 

              The data used to train the model is generated from the text of interest.  This could be all articles on wikipedia or, in our case, a corpus of scientific papers.
              We define a sliding window that scans through the text generating training samples for the model.  
              So if your window is 5 words long, then the first training sample will be the first 5 words of your text data, and the second will be words 2-6 and so on.<br><br>

              The ultimate goal will be for the model to predict whether or not words are neighbors (appear together in the text).
              To do this, we generate positive and <a class="outlink" href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/">negative samples</a> for each word in the text.  
              The positive samples are words that are actually neighbors, and the negative samples are random words from the vocab which are probably not neighbors. 
              There are two possible approaches for generating the positive samples: 
              <a class="outlink" href="https://jalammar.github.io/illustrated-word2vec/">a continuous bag-of-words model (CBOW) and continuous skip-gram model.</a><br><br>
              For this project, we use the CBOW model with a window size of 11 and 14 negative samples per positive sample.

              This explanation will use the example with a window of size 5 where the current window is "drinking coffee wakes me up".<br><br>
            </p>
            
            <img src="figures/w2v_window.PNG", alt="word2vec window example" style='height: 30%; width: 30%; object-fit: contain; margin: 1% 34%'>
            
            <p><br><br>
              In the continuous bag-of-words model (CBOW), each training sample consists of one focus word, your window size (<span class="emp">n</span>) minus 1 context words
              and a target value which is 1 for positive samples and 0 for negative samples.
              The focus word is at the center of the sample. So for our example, the positive sample will have <br>
              Pos: focus = ["wakes"] context = ["drinking", "coffee", "me", "up"] target = 1. <br>
              A negative sample might have<br> 
              Neg: focus = ["dentist"] context = ["drinking", "coffee", "me", "up"] target = 0. <br><br>

              After generating these samples, the window would move one word to the right, and the process repeats.
              <br><br>

              Now that we know what our data looks like, we can build some intuition as to how our model learns word embeddings.
              To begin, we define the vocabulary size (<span class="emp">M</span>) and the dimension of the word embeddings (<span class="emp">N</span>), and we initialize the weights randomly.
              The neural network is composed of an input layer of size <span class="emp">M</span>, a hidden layer of size <span class="emp">N</span>, and an output layer of size <span class="emp">M</span>.<br><br>
            </p>

            <img src="figures/w2v_network.PNG", alt="word2vec network" style='height: 40%; width: 40%; object-fit: contain; margin: 1% 29%'>
            
            <p><br><br>
              For the forward step, we take one positive sample and multiple negative samples for the same focus word.
              We essentially compute the dot product between the word embeddings of the focus word and context word for each example, and apply a 
              <a class="outlink" href="https://cs231n.github.io/linear-classify/#softmax">softmax fuction</a> to it so that the output represents the probability of each sample being true neighbors.<br><br>

              We can compute a loss or error by comparing the difference between the target value (1 for true neighbors and 0 for false neighbors) and the probability generated by the model. 
              We then use this error in the <a class="outlink" href="https://purnasaigudikandula.medium.com/a-beginner-intro-to-neural-networks-543267bda3c8">backpropagation step</a> to update the weights.
              The first layer of weights is ultimately our word embedding.<br><br>
              </p>

              <button type="button" class="collapsible"> Read More </button>
              <div class="extended" id="w2v_more">
                <p> <br>
                  The input layer has a <a class="outlink" href="https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f">one hot encoding</a> 
                  of the context words meaning that all values are 0 except at an indexes representing that context words where the value is 1.
                  There is a connection between each input neuron and all neurons in the hidden layer, and each of those connections has a weight.
                  The weights on the connections between each input word and all <span class="emp">N</span> hidden neurons represents the embedding for that word.<br><br>
                </p>

                <img src="figures/w2v_extra_fig.PNG", alt="w2v_network_fig" style='height: 60%; width: 60%; object-fit: contain; margin: 1% 19%'>

                <p><br><br>
                  On the forward pass, we multiply each input neuron's value (all 0 except this example's context words which each have a value of 1) by the weight of its connection, and
                  sum these values for all incoming connections to each hidden neuron.  In skip-gram, the hidden neuron's value will always just be the weight of its connection to
                  the input word and will therefore be the word embedding.  We apply an <a class="outlink" href="https://purnasaigudikandula.medium.com/a-beginner-intro-to-neural-networks-543267bda3c8">activation function</a> 
                  to this value and then do the same thing for the next layer.  This means that each neuron in the output layer is now equal to the weighted sum of each value 
                  of the hidden layer where the weights are defined by the connections between two neurons. 
                  From there, we apply a softmax function <a class="outlink" href="https://cs231n.github.io/linear-classify/#softmax">softmax fuction</a> which allows us to interpret each output neuron value as a probability.
                  Similar to the input layer, each neuron in the output layer represents a word in the vocabulary, so each output neuron value represents the probability of that word
                  being a neighbor of our input words.<br><br>
    
                  We can compute a loss or error by comparing the difference between the target value (1 for true neighbors and 0 for false neighbors) and the model generated probability.
                  Rather than backpropagating all of the error and updating all of the weights, we only update the weights for the one positive example and 
                  <a class="outlink" href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/">multiple negative examples</a>.
                  Backpropagating the error updates the weights and therefore updates the word embedding making it more likely to correctly predict neighboring words.
                  In the figure above, our context words are emphasized on the input side.  On the output side, the positive sample (focus word) is emphasized as well as
                  a small set of negative samples.  Only these values are used for calculating the loss and backpropagation. 
                </p>
              </div>
    
              <p><br>
                This is repeated for every word in the data, and the whole data set is run through multiple times. 
                Once training is completed, we can take our word embeddings and plot them or perform operations on them and gain insight about the relationships between words in our dataset.<br><br>

                This work was largely inspired by the work of Vahe Tshitoyan et. al in 2019 
                <a class="outlink" href="https://www.nature.com/articles/s41586-019-1335-8">Unsupervised word embeddings capture latent knowledge from materials science literature</a>.
              </p>

          </div>
          
          <p><br><br>
            <span class="bolded"> t-distributed Stochastic Neighbor Embedding (tSNE) </span>
            is a dimensionality reduction technique that projects points in higher dimensions into a lower dimensional space.
            Each of the word vectors are 100 dimensional, but we want to view the vectors on a 2D screen.
            There are many different ways you could project down to 2D.
            Since we're interested in comparing the similarity of our words (how close they are in vector space), 
            we want to find a projection that preserves the relative distance between points.
            tSNE calculates a similarity measure between points in high dimensional space 
            and then finds the optimal projection to lower dimensional space that preserves the neighborhood of each point.<br><br>
          </p>
          
          <a href="https://www.kdnuggets.com/2017/11/3-different-types-machine-learning.html/2">
            <img src="figures/dim_reduct.jpg", alt="dimensionality reduction" style='height: 50%; width: 50%; object-fit: contain; margin: 1% 24%'>
          </a>
          
          <button type="button" class="collapsible"> Read More </button>
          <div class="extended" id="tSNE">
            <p><br>
              Often times, data science and machine learning involves dealing with high dimensional vectors which can be difficult to work with and to visualize.
              It's often the case that the majority of the information stored in the data exists in a lower dimensional representation of the data.
              In our case, we want to find a projection of our 100 dimensional vectors onto a 2D plot while preserving the most important information: 
              the relative neighborhood of each point.
              Word vectors that are close together (similar) in high dimensional space should appear close together on our 2D plot.<br><br>
              
              To accomplish this, we use <a class="outlink" href="https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf">t-distributed Stochastic Neighbor Embedding (tSNE)</a>
              proposed by Geoffrey Hinton and Laurens van der Maaten in 2008.
              This algorithm can be thought of in three steps where the second and third step repeat iteratively until the optimal projection is found.<br><br>

              <span class="bolded">Step 1. Determine the similarity of all points in high dimensional space.</span><br>
              The objective is to preserve the neighborhood in high dimensional space when we project to lower dimensions.
              For each point, we calculate its "similarity" to all other points in our data in high dimensions.
              To do this, we center a <a class="outlink" href="https://machinelearningmastery.com/statistical-data-distributions/">Gaussian distribution</a> 
              around our point of interest that represents the probable distance to all other points.
              We can adjust the perplexity parameter to change the expected density around each point.<br>
            </p>
           
            <a href="https://www.youtube.com/watch?v=NEaUSP4YerM">
              <img src="figures/tsne_step1.PNG", alt="tsne step 1" style='height: 50%; width: 50%; object-fit: contain; margin: 1% 24%'>
            </a>
            
            <p><br><br>
              We normalize the distances to get probabilities that are proportional to the similarity of two points.
              Because two different points can have two distributions, the probability of <span class="bolded">a --> b</span> 
              might not equal the probability of <span class="bolded">b --> a</span>.
              The final similarity is equal to the average of the paired probabilities divided by the number of data points.<br><br>
            </p>
            
            <img src="figures/tsne_pij_eq.PNG", alt="tsne equations" style='height: 45%; width: 45%; object-fit: contain; margin: 1% 27%'>
            
            <p><br>
              Where <span class="bolded">x</span><span class="sub">i</span> and <span class="bolded">x</span><span class="sub">j</span> are distinct points in high dimensional space,
              <span class="bolded">N</span> is the number of data points, 
              and <span class="bolded">&sigma;</span><span class="sub">i</span> is the perplexity of the <span class="bolded">i</span>th Gaussian kernel.
              The end result is a distribution of the similarity scores between each point in high dimensional space which we will call <span class="bolded">P</span>.<br><br>
              
              <span class="bolded">Step 2: Naively project data to lower dimensional space and calculate new similarity score in lower dimensions.</span><br>
               Once we have the higher dimension similarities, we project our points down to lower dimensional space (our end goal space).
               Here again, we compute similarity scores between each point, but instead of using a Gaussian kernel, 
               we use a <a class="outlink" href="https://machinelearningmastery.com/statistical-data-distributions/">student t distribution</a> with 1 degree of freedom.
               The student t distribution has more weight in the tails which helps to prevent clusters from clumping in the center.
               Just like in higher dimensions, we normalize the similarities.
            </p>

            <img src="figures/tsne_qij_eq.PNG", alt="tsne equations" style='height: 35%; width: 35%; object-fit: contain; margin: 1% 32%'>
            
            <p>
              Where <span class="bolded">y</span><span class="sub">i</span> and <span class="bolded">y</span><span class="sub">j</span> are distinct points in low dimensional space,
              The end result is a distribution of the similarity scores between each point in low dimensional space which we will call <span class="bolded">Q</span>.<br><br>
              
              <span class="bolded"> 3. Use the KL divergence score as a loss function and gradient descient to iteratively improve the projection. </span><br>
              Since the objective is to preserve the similarity between points, we use the 
              <a class="outlink" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler divergence</a> of <span class="bolded">P</span>
              and <span class="bolded">Q</span> to measure how much our similarity distributions differ between high dimensional space and low dimensional space.
              This is used as a loss function for <a class="outlink" href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/">gradient descent</a> allowing 
              us to iteratively adjust our points in low dimensional space to make the similarity distribution closer to the high dimensional distribution.
              <br>
            </p>

            <img src="figures/KL.PNG", alt="Kullback-Leibler Divergence" style='height: 25%; width: 25%; object-fit: contain; margin: 1% 37%'>

            <p>
            By minimizing the KL divergence of the distributions <span class="bolded">P</span> and <span class="bolded">Q</span> with respect to each point in our projection
            <span class="bolded">y</span><span class="sub">i</span>, we can produce a projection that optimally preserves high dimensional similarities.
            Put simply, we systematically adjust the location of our points in low dimensional space, calculate the new similarity distribution 
            <span class="bolded">Q</span>, and compare it to our high dimensional similarity distribution <span class="bolded">P</span> 
            until the two distributions are as close as we can get them.
            </p>

          </div>
        </div>

        <p><br>
          <span class="fieldname">Results: </span>
          We found this visualization a bit more difficult to interpret, but still potentially useful.
          The words most similar to the selected words seem to make clear sense, but this isn't always reflected in the 
          two dimensional projection.  We've found a number of word equations that produce meaningful results, but it takes
          some time to develop intuition for how to build word equations.  Specific results are listed below.
        </p>
        
        <button type="button" class="collapsible"> Read More </button>
        <div class="extended">
          <p><br>
            We found that the similar words generally make a lot of sense.
            Some examples of then ten nearest words are listed below.<br>
          </p>

          <p><br><br>            
            <span class="bolded">batteri:</span> pb_acid, recharg, li, stationari, super-capacitor, lib, ultracapacitor, cell, pack, bm<br>
            <span class="bolded">cost:</span> bill, invest, ghg_emiss, consumpt, price, expens, payment, incom, lcoe, penalti<br>
            <span class="bolded">solar:</span> sun, photovolta, sunlight, parabol_trough, pvt, csp, plant, flat_plate, geotherm, panel<br>
            <span class="bolded">geotherm:</span> subsurfac, geolog, ocean, ute, aquif, ground, underground, tidal, groundwat, shallow<br>
            <span class="bolded">li:</span> na, lib, zn, cathod, recharg, batteri, multival, anod, li-, alkali<br><br>

            Some word equations we found are shown below along with some
            interpetations of why the word equation might make sense. 
            <br><br>
            The way to read these equations is: 
            <br>
            A + B - C = D
            <br>
            As C is to A, B is to D. 
          </p>
            
          <img src="figures/batt+h_li.PNG" alt="batteri+h-li=electrolys" style='height: 50%; width: 50%; object-fit: contain; margin: 1% 24%'>
          <p><br>
            An electrolyzer stores electrical energy in the form of chemical
            energy by producing hydrogen from electricity.This is similar way to
            how a battery stores electrical energy in the form of chemical
            energy by moving Lithium from one electrode to another with
            electricity<br>
          </p>

          <img src="figures/salt+chem_therm.PNG", alt="molten_salt+chemic-thermal=synthet" style='height: 50%; width: 50%; object-fit: contain; margin: 1% 24%'>
          <p><br>
            Molten salt is a thermal storage medium and synthetic fuels(hydrogen/hydrocarbons) are a chemical storage medium.<br>
          </p>

          <img src="figures/cost+cyc-effic.PNG", alt="cost+cycle-effici=long_term" style='height: 50%; width: 50%; object-fit: contain; margin: 1% 24%'>
          <p><br>
            The efficiency is to the cost as the cyclability is to the lifetime.<br>
          </p>

          <!-- <img src="figures/overpot+g1-macm2.PNG", alt="overpotenti + g1 - ma_cm2 = cm_2" style='height: 50%; width: 50%; object-fit: contain; margin: 1% 24%'> -->
          <p><br>

            This process doesn't always work as expected.  For instance<br> 
          </p>
          
          <img src="figures/colio2+anod-cathod.PNG", alt="colio2 - cathod + anod = felio4p" style='height: 50%; width: 50%; object-fit: contain; margin: 1% 24%'>
          <p><br>
            We expect that this would produce an anode material for lithium batteries, but it instead producees primarily cathode materials.<br><br>

            These are all promising results, but more exploration is required to realize the full potential of this tool.
          </p>

        </div>
        
        <p> <br>
          <span class="fieldname"> Visuals: </span> 
          <a class="btn" href="html_files/W2V_tSNE.html"> Word2Vec Plot </a>
        </p>
      </div>
    </section>
    <section>
      <p>
        Work done by Bryce Elizabeth Yahn and Lee Aspitarte in 2021.  Built upon work by Raihan Ahmed and Lee Aspitarte in 2020.<br><br>
      </p>

      <button type="button" class="collapsible"> Bios </button>
      <div class="extended">
        <p>
          <span class="bolded"> Lee Aspitarte </span> received his Bachelor of Science in physics from Washington State University, Washington, U.S., in 2011 and his 
          PhD in physics from Oregon State University, Oregon, U.S., in 2017. From July to October of 2017, he was a Postdoctoral Researcher with the National Energy Technology 
          Laboratory. Since 2017, he has been a Principal Research Scientist with Battelle, under the U.S. Department of Energy, at the National Energy Technology Laboratory
           in Albany, Oregon. His interests include experimental research in the field of magnetohydrodynamic power generation and its associated subfields. 
          Dr. Aspitartes awards and honors include the Ben and Elaine Whiteley Materials Research Fellowship (Oregon State University) and the Peter Fontana Outstanding Graduate 
          Teaching Assistant Award (Oregon State Department of Physics).<br><br>

          <span class="bolded"> Bryce Elizabeth Yahn </span> Bryce is a recent graduate of University of Rochester where she earned a bachelor of science in brain and cognitive sciences 
          with a minor in mathematics.  She has participated in four research programs on her journey from studying the brain to working in the field of data science and artificial intelligence.  
          She has conducted neuroscience research at the MIT Summer Research program, studied eye movements and attention in Berlin at Humboldt Universitat, and completed machine vision research 
          at University of Rochester.  She is now gaining experience with natural language processing and data visualization working with the Department of Energy to better understand the vast 
          body of scientific literature relating to renewable energy storage.  In September, she will be moving to Arlington, VA to start a data science position working in Leidoss AI and 
          machine learning accelerator group.  She is passionate about making STEM an inclusive and welcoming field for everyone and hopes to continue working towards that cause in every position 
          she takes.  Outside of work, she is an avid rock climber, and likes to push herself to new heights by facing her fears and challenging herself every day.  She is also interested in 
          photography.  She won a scholarship for a tuition free fifth year of college to study how photography can shape our perspectives and world views.<br><br>
          
          <span class="bolded"> Raihan Ahmed </span> was a rising senior pursuing his Bachelor of Science in computer science and minor in linguistics at Northeastern Illinois 
          University in Chicago, IL at the time of this work. Raihan intended to pursue a PhD in computer science or linguistics. From 2017 to 2018, he was an undergraduate 
          research assistant at the University of Illinois at Urbana-Champaigns (UIUC) College of Agricultural, Consumer, and Environmental Sciences, as well as a teaching 
          assistant and academic tutor with UIUCs Department of Physics. Between 2018 and 2019, he was an information systems and technology co-op student with Apple. 
          Since then, he has been a research fellow at the National Energy Technology Laboratory in Albany, Oregon. Mr. Ahmeds awards and honors include the Mickey Leland 
          Energy Fellowship (Oak Ridge Institute for Science and Education).<br><br>
        </p>
      </div>
      <br><br>
      <button type="button" class="collapsible"> References</button>
      <div class="extended">
        <p><br>

        Ammar, W., Groeneveld, D., Bhagavatula, C., Beltagy, I., Crawford, M., Downey, D., Dunkelberger, J., Elgohary, A., Feldman, S., Ha, V.A., Kinney, R.M., Kohlmeier, S., Lo, K., Murray, T.C., Ooi, H., Peters, M.E., Power, J.L., Skjonsberg, S., Wang, L.L., Wilhelm, C., Yuan, Z., Zuylen, M.V., & Etzioni, O. (2018). Construction of the Literature Graph in Semantic Scholar. ArXiv, abs/1805.02262.<br><br>

        Bickel, M.W. Reflecting trends in the academic landscape of sustainable energy using probabilistic topic modeling. Energ Sustain Soc 9, 49 (2019). https://doi.org/10.1186/s13705-019-0226-z<br><br>

        David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res. 3, null (3/1/2003), 9931022.<br><br>

        Blondel, V. D., Guillaume, J., Lambiotte, R., & Lefebvre, E. (2008). Fast unfolding of communities in large networks. Journal of Statistical Mechanics: Theory and Experiment, 2008(10). doi:10.1088/1742-5468/2008/10/p10008<br><br>

        Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. ICLR.<br><br>

        Tshitoyan, V., Dagdelen, J., Weston, L. et al. Unsupervised word embeddings capture latent knowledge from materials science literature. Nature 571, 9598 (2019). https://doi.org/10.1038/s41586-019-1335-8<br><br>

        Van der Maaten, L., Hinton, G. (2008). Visualizing High-Dimensional Data Using t-SNE. Journal of Machine Learning Research 9(Nov):2579-2605, 2008.<br><br>

        All references are linked where mentioned and all images taken from other cites have links to those sites.
        Additional links to useful blog posts are included for terms that may not be familiar to all researchers.
        </p>
      </div>
    </section>
    <!--This section in the tag <script> is the javascript for opening and closing the collabsibles when you press the button.  it changes the content display from block to none and back.-->
    <script>
      var coll = document.getElementsByClassName("collapsible");
      var i;
      
      for (i = 0; i < coll.length; i++) {
        coll[i].addEventListener("click", function() {
          this.classList.toggle("active");
          var content = this.nextElementSibling;
          if (content.style.display === "block") {
            content.style.display = "none";
            console.log('hide');
          } else {
            content.style.display = "block";
            console.log('display');
          }
        });
      }
    </script>
  </body>
</html>
