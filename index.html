<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>MLEF Energy Storage Visuals</title>
    <link rel="stylesheet" href="main.css">
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500&display=swap" rel="stylesheet">
  </head>
  <body>
    <header>
      <h1>
        Energy Storage Topic Modeling <br>
      </h1>
    </header>
    <section class="proj">
      <h2> The Problem </h2>
      <p>
        Storing energy from intermittent renewables, such as wind and solar, is one of the most pressing challenges we face for enabling a sustainable civilization. 
        A wide array of energy storage technologies are under development, each with their own advantages and disadvantages for various use cases. 
        Scientific research into energy storage technologies has exploded in recent years and sorting through this large body of knowledge to understand the state of field 
        is an important and challenging problem.<br>      
      </p>

      <button type="button" class="collapsible"> Read More </button>
      <div class="extended">
        <p><br> 
          As the need for clean, sustainable energy has become more imperative, the amount of research in the field of energy storage has skyrocketed.<br>
        </p>
           <img src="Years.png" alt="Timeline of publications" class="center_fig" style='height: 70%; width: 70%; object-fit: contain'>
        <p>
          The plot above shows the distribution of publication year for the papers in our dataset, and it clearly demonstrates that the body of research is expanding at an unprecidented rate.
          While this increase is a good thing, it makes it very difficult to maintain a high-level understanding of the state of the field and to stay aware of what progress is being made.
          This lack of insight can lead to redundant research, inefficient investing of time and money, and oversight of areas with the greatest potential.
          To optimize effective allocation of resources, investigators and administrative bodies need a broad understanding of the current state of the field including
          different areas of study and their connections as well as trends in areas of interest.
        </p>
      </div>

      <h2> <br> The Goal </h2> 
      <p>
        This project uses a variety of natural language processing techniques to extract insights from the scientific literature with the goal of being able to better direct research 
        efforts and investments towards promising technologies.<br>
      </p>
      <button type="button" class="collapsible"> Read More </button>
      <div class="extended">
        <p> <br>
          While it's nearly impossible for any individual to read through the tens of thousands of papers found in our dataset, recent AI technology has the power to process all this 
          data and more and to extract insights without the constraints of human pre-conceived ideas of what is or is not important.  
          Furthermore, the machine learning techniques we use may find patterns and connections between different areas of the field that humans might not be able to recognize on their own.<br><br>
          We have ultimately generated two interactive plots to allow researchers to explore the learned models of the field of energy storage.  (More details on techniques in the sections below)
          The first plot shows a graph of topics extracted from the abstracts in our dataset.
          Researchers can click on topics and find related papers and information about topic trends.  They can also look at the connections between topics and find papers and trends at the intersection of topics.
          The second plot zooms in further to look at the relationships between individual key words in the data.
          It allows researcher to explore embedded words from our data projected onto a 2D plane, and use word-vector math to look at relationships between different words (details on this in the Word2Vec section).
        </p>
      </div>

      <h2> <br> The Data </h2>
      <p>
        The data is composed of ~60,000 abstracts of scientific papers related to energy storage.  
        The papers were pulled from the Semantic Scholar Open Research Corpus database of scientific papers for natural language processing.<br>
      </p>
      
      <button type="button" class="collapsible"> Read More </button>
      <div class="extended">
        <p> <br>
          Our models were trained on 54,125 abstracts from scientific papers relating to energy storage pulled from the Semantic Scholar Open Research Corpus.
          The corpus is a collection of research papers published in all fields in a JSON archive designed for ease of use in natural language processing tasks.
          The corpus is rich with metadata including the fields of study the abstract is from, authors, years published, and the in-citations 
          (papers that cite the current paper) and the out-citations (papers cited by the current paper).  The plots below give an overview of these features for our dataset.<br><br>
        </p>
        <img src="FOS.png" alt="Fields of Study Figure" class="center_fig" style='height: 70%; width: 70%; object-fit: contain'>
        <div> 
          <img src="Length.png" alt="Length of Abstract" style='height: 30%; width: 30%; object-fit: contain'>
          <img src="InCite.png" alt="Number of external citations" style='height: 30%; width: 30%; object-fit: contain'>
          <img src="OutCite.png" alt="Number of references per paper" style='height: 30%; width: 30%; object-fit: contain'>
        </div>
        <p> <br>
          To cultivate our dataset, we pulled all abstracts from the entire semantic scholar dataset with the search term "Energy Storage" in the title or abstract. (extract_papers_semantic.py).
          Then we processed the the raw text.  First, we excluded papers in any language that wasn't english.  
          Then we removed stop words, which are commonly used words in the english language that carry very little useful information for natural language processing such as (the, a, is, are) and punctuation.
          Since many of the papers in our database are related to chemical and material engineering, we used the
          <a href="https://github.com/materialsintelligence/mat2vec#supplementary-materials-for-unsupervised-word-embeddings-capture-latent-knowledge-from-materials-science-literature-nature--571-9598-2019"> mat2vec</a> processing library
          to standardize the representation of checmical formulas. For example, converting all instances of "Lithium" to "li" to avoid redundancy.
          We used WordNetLemmatizer which converts words to their meaningful base form while also considering the context.
          We also used porter stemming which reduces all variations of words to one root.  This is effective, though may limit interpretability in some cases.  
          For example, the words "cycle", "cyclic", "cycled", and "cycling" would all be converted to "cycl".
          The result is the processed text we use to train our models.
        </p>
      </div>
            
      <h2>
        <br>To see our code, please visit our    
        <a class="gitlink" href="https://github.com/MLEF-Energy-Storage"> GitHub </a>
      </h2>
    </section>
    
    <section id="LDA">
      <h2>
        Topic Modeling with LDA and Louvian Communities
      </h2>
      <div class="photo">
        <img src="lda.PNG" alt="lda" class="picture" style='height: 100%; width: 100%; object-fit: contain'>
      </div>
      <div class="description">
        <p>
          <span class="fieldname"> Description: </span> 
          
          Topic modeling is a machine learning problem where you have a collection (corpus) of documents, and you want to extract topics from the unstructured text data.
          In our case, our corpus is the set of abstracts pulled from Microsoft Academic, and we want to extract topics of interesting research from these texts.
          Once we've extracted these topics, we create a graph of topics where each node is a topic and each edge represents the probability of these topics co-occuring in an abstract.
          Different features of the graph convey further information about how prevelant a topic is or how frequently it has shown up in abstracts in the past five years.<br><br>

          <span class="fieldname"> Goal: </span> 
          
          The goal of this modeling and visualization is to algorithmically produce a high-level overview of the field and 
          to allow users to interactively explore different topics and the connections between them.
          We hope this provides insight into surprising connections between different areas of the field or trending topics of interest.
          This visualization can also be used to explore literature in the field related to each topic or their connections.<br><br>
          
          <span class="fieldname"> Algorithms: </span> 
          
          This visualization relies primarily on two techniques: Latend Dirichlet Allocation and Louvain Community Detection.<br><br>
        </p>
    
        <button type="button" class="collapsible"> Read More </button>
        <div class="extended">
          <p><br>
            <span class="bolded"> Latent Dirichlet Allocation (LDA) </span> is a popular method used for topic modeling.
            It models topics as probability distributions over words. <br> 
            (<span class="emp"> ex: the topic of "pets" has a high probability of containing the word "cat" and a low probability of containing the word "solar_panel". </span>)<br>
            Topics can be represented by the words that have the highest probability of belonging to the topic. Furthermore, each document has a certain probability of containing each topic.  <br>
            (<span class="emp"> ex: An article about healthy pet diets has a high probability of containing the topics "food", "pets", and "health" and a low probability of containing the topic "energy_storage."</span>)<br>
            We performed topic modeling with 100 topics and then produced a graph where nodes represent topics and edges represent the probability of topics occuring in the same document.<br>
            (<span class="emp"> ex: the topics "health" and "food" are more likely to co-occur than "pets" and "energy_storage".</span>) <br>
          </p>
          
          <button type="button" class="collapsible"> Read More </button>
          <div class="extended" id="lda">
            <p> <br>
              Latent Dirichlet Allocation is a generative probabilistic model often used for topic modeling.  It was first applied to machine learning 
              by David Blei, Andrew Ng, and Michael Jordan in their 2003 paper <a href="https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf"> Latent Dirichlet Allocation </a>.
              A generative probabilistic model simulates the random production of some outcome by defining a graph in which the nodes are conditionally 
              dependent random variables and the edges represent their relationship.  <br>
              For instance, if we wanted to simulate whether or not you wear a coat each day for a week, we might model it using the graph below.<br>
            </p>
            <img src="gen_prob_model.PNG" alt="generative probabilistic model" class="center_fig" style='height: 50%; width: 50%; object-fit: contain'>
            <p> <br>
              Each day, there's some probability of it being cloudy or sunny.  Depending on that, there's some probability of it being cold or warm.
              And depending on that, there's some probability of you wearing a coat.  
              If we know the exact probability distributions, we can generate a simulation of your coat wearing habits.
              However, it's often difficult to know all of the probability distributions.
              Often, the only observable part of the graph is the outcome (in this example, whether or not you're wearing a coat).
              In this case, we can apply sampling methods and <a href="http://mathcenter.oxford.emory.edu/site/math117/bayesTheorem/"> Bayes theorem </a> to use the observable variables to estimate the underlying distributions for the latent variables (unobservable variables).<br><br>
              
              Using a more complex generative model, we can generate a simplified simulation of writing scientific papers.
              The model used in LDA is a three tier hierarchical Bayesian model. The boxes are "plates" representing repeated entities such as multiple papers, words, and topics.<br>
            </p>
            <img src="LDA_plate.png" alt="LDA plate model" class="center_fig" style='height: 50%; width: 50%; object-fit: contain'>
            <p><br>
              The only observable variable is the words in each paper.  The latent variables are the topics in the corpus of papers.
              In this model, each topic is a <a href="http://mathcenter.oxford.emory.edu/site/math117/multinomialDistribution"> multinomial distribution</a> over words with parameter &phi; representing the probability of each word being used in that topic.
              Each paper is a multinomial distribution over topics with parameter &theta; representing the probability of each topic being used in that document.
              So for each document <span class="bolded">M</span> in our corpus, there is a probability of it including topic <span class="bolded">Z</span> which in turn has a probability of including each word <span class="bolded">N</span> in the document.<br><br>
              
              Taking it one step further, we model the parameters of our multinomial distributions using <a href="https://towardsdatascience.com/dirichlet-distribution-a82ab942a879"> Dirichlet distributions</a>.
              In order to get the parameters &phi;<span class="sub">1</span>...&phi;<span class="sub">K</span> representing the probability of each word being used in topics 1-<span class="bolded">K</span>, we use a Dirichlet distribution with parameter &beta;.
              We use another Dirichlet distribution with parameter &alpha; to get the parameters &theta;<span class="sub">1</span>...&theta;<span class="sub">M</span> representing the probability of each topic being used in documents 1-<span class="bolded">M</span>.
              The Dirichlet distributions is the <a href="https://towardsdatascience.com/conjugate-prior-explained-75957dc80bfb"> conjugate prior </a> for the multinomial distribution parameters.  For our model, we set &alpha; as 1/num_topics and &beta; as 0.03.<br><br>

              Now that we have our model, we can reverse engineer it using the words to infer the topics.
              To do this, we use <a href="https://towardsdatascience.com/gibbs-sampling-8e4844560ae5"> Gibbs sampling </a> which is a Markov chain Monte Carlo algorithm for approximating multivariate probability distributions.
              This process approximates the joint distribution for all random variables in our plot.
              At the end of this process, we can define each topic by the top most probable words for that topic.
              We can also keep track of the documents with the highest probability of containing each topic as well as the probability of two topics co-occuring in the same document.
              This information can be used to define a graph in which the nodes are topics and the edges are weighted probabilities of topics co-occuring.
            </p>
          </div>

          <p> <br><br> 
            <span class="bolded"> Louvain Community Detection </span>
            The Louvain method for community detection is used on large intereconnected graphs to separate nodes into related groups or communities.
            It starts out by defining each node as an individual community.
            It then uses a similarity metric to compare adjacent nodes, and if they are similar enough, it merges the two communities.
            When it has gone through every node, it creates a new graph in which each community is a single node.
            It repeats this process until no nodes are similar enough to merge.
            The end result is a grouping of similar nodes into research communities.
            These are represented by the colors of the nodes in our graph.<br>
          </p>
          
          <button type="button" class="collapsible"> Read More </button>
          <div class="extended" id="louvain">
            <p><br> 
              The Louvain method for community detection seeks to extract communities from large networks.
              The method was invented by Blondel et al. in their 2008 paper <a href="https://arxiv.org/pdf/0803.0476.pdf"> Fast unfolding communities in large networks</a>. 
              The communities are sets of highly interconnected nodes such that connections within communities are dense while connections between communities are sparse.
              Precise optimization for this problem is computationally intractable... <br><br>
              
              The algorithm has two iterative phases.
              To begin, each node in the network is assigned a different community.
              In the first step,
              For each node, <span class="bolded">i</span>, the gain modularity is calculated for the event in which <span class="bolded">i</span> 
              is removed from its community and placed in the community of each of its neighbors <span class="bolded">j</span>.
              The modularity is a value in the range of [-1/2,1] measuring the ratio of edge density inside vs between communities defined as
            </p>
            <img src="modularity.png" alt="modularity formula" class="center_fig" style='height: 30%; width: 30%; object-fit: contain'>
            <p> <br>
              Where<br>
              <span class="bolded">m</span> is the total sum of edge weights in the graph<br>
              <span class="bolded">A</span><span class="sub">ij</span> is the edge weight between nodes <span class="bolded">i</span> and <span class="bolded">j</span><br>
              <span class="bolded">k</span><span class="sub">i</span> and <span class="bolded">k</span><span class="sub">j</span> are the sums of the weights of the edges 
              connected to nodes <span class="bolded">i</span> and <span class="bolded">j</span> respectively.<br>
              <span class="bolded">&delta;</span> is the Knonecker delta function where <span class="bolded">&delta;</span>(x,y)=1 if x=y and 0 otherwise.<br>
              <span class="bolded">c</span><span class="sub">i</span> and <span class="bolded">c</span><span class="sub">j</span> are the communities of the 
              nodes <span class="bolded">i</span> and <span class="bolded">j</span> respectively.<br><br>

              The modularity gain is the change in modularity defined by<br>
            </p>
            <img src="modularity_gain.png" alt="modularity gain formula" class="center_fig" style='height: 60%; width: 60%; object-fit: contain'>
            <p><br>
              Where <br>
              <span class="bolded">&Sigma;</span><span class="sub">in</span> is the sum of the weights of the edges inside the community <span class="bolded">C</span>that <span class="bolded">i</span> is joining<br>
              <span class="bolded">k</span><span class="sub">i,in</span> is the sum of the weights of edges from <span class="bolded">i</span> to nodes in <span class="bolded">C</span><br>
              <span class="bolded">m</span> is the total sum of edge weights in the graph<br>
              <span class="bolded">&Sigma;</span><span class="sub">tot</span> is the sum of the weights of the edges incident to nodes in the <span class="bolded">C</span><br>
              <span class="bolded">k</span><span class="sub">i</span> is the sum of the weights of edges incident to node <span class="bolded">i</span><br><br>
            
              If the max gain is positive, then the node <span class="bolded">i</span> is placed in the community with maximum gain.
              Otherwise <span class="bolded">i</span> remains in its own community.<br><br>

              In the second phase, a new network is defined in which the nodes are the communities found during the previous phase.
              The new edge weights are set to the sum of the weights of the edges between the two communities in the previous network.
              Edges between nodes in the same community become self loops in the new network.<br><br>

              Once the second phase is complete, the first phase is repeated on this new graph, and the process repeats until there is no positive modularity gain from combining communities.<br>
              The Louvain method for community detection runs in <span class="emp">O(nlogn)</span> time where <span class="emp">n</span> is the number of nodes in the network.

            </p>

          </div>
        </div>
                    
        <p> <br>
          <span class="fieldname"> Visuals: </span> 
          <a class="btn" href="ES_networkplot_red.html"> Energy Storage Network Plot </a>
        </p>
        
      </div>
    </section>
    <section id="w2v">
      <h2>
        Word2Vec Visualized with PCA
      </h2>
      <div class="photo">
        <img src="w2v.png" alt="w2v" class="picture" style='height: 100%; width: 100%; object-fit: contain'>
      </div>
      <div class="description">
        <p>
          <span class="fieldname"> Description: </span> 
          In natural language processing, words are often embedded into a numerical vector that encodes the meaning of the words.
          The embeddings are learned from the context and surrounding words using machine learning algorithms.
          These word vectors can be conceptualized as a point in high dimensional space.
          We can easily compare how close these vectors are in high dimensional space, giving us a good idea of how similar the words are.
          We then projected these words onto a two dimensional plane so that we could spacially visualize the relationships between different words.
          Another cool feature of word vectors is that you can do math with them.  
          By adding and subtracting vectors, you can search for relevant words.<br>
          <span class="emp"> ex: if you take the embedding of "King" and subtract the embedding of "Man", the resultant vector represents the relationship between "King" and "Man".
          If you then add this new vector to the embedding of "Woman", the resulting vector is most similar to the embedding of "Queen".</span><br><br>
        </p>
        <p>
          <span class="fieldname"> Goal: </span> 
          The goal of this visualization is to be able to choose words out of the collection of abstracts and explore their relationships either by
          seeing how close they are on the two dimensional visualization, or my experimenting with vector math. <br><br>
        </p>
        
        <p>
          <span class="fieldname"> Algorithms: </span> 
          This visualization relies primarily on two techniques: Word2Vec and Principal Component Analysis.<br><br>
        </p>

        <button type="button" class="collapsible"> Read More </button>
        <div class="extended">
          <p><br>
            <span class="bolded"> Word2Vec </span>
            Word2vec is a word embedding technique that uses a neural network to learn word associations from a large corpus of text.
            For each word, it uses the current word embedding to try to predict the words in some window around it.
            When it incorrectly predicts a word, it uses the error to update the word embedding and improve.
            The end result is a word embedding for each word in the corpus that is learned from the context of surrounding words.
            <br>
          </p>
          
          <button type="button" class="collapsible"> Read More </button>
          <div class="extended" id="word2vec">
            <p> Even more in depth discussion of Word2Vec. </p>
          </div>
          
          <p><br><br>
            <span class="bolded"> Principal Component Analysis (PCA) </span>
            is a dimensionality reduction technique that projects points in higher dimensions into a lower dimensional space.
            Each of the word vectors are 200 dimensional, but we can visualize at most 3 dimensions, and in this case, we want to view the vectors on a 2D screen.
            There are many different ways you could project down to 2D, but we want to choose the one that preserves as much of the initial information as possible.
            PCA uses linear algebra to find the projection that preserves the most variance/relevant information so that we can visualize these 200D vectors on a 2D screen without losing too much information.
            <br>
          </p>
          
          <button type="button" class="collapsible"> Read More </button>
          <div class="extended" id="pca">
            <p> Even more in depth discussion of PCA. </p>
          </div>
        </div>
          
        <p> <br>
          <span class="fieldname"> Visuals: </span> 
          <a class="btn" href="W2V_PCA.html"> Word2Vector Plot </a>
        </p>
      </div>
    </section>
    <section>
      <p>
        Work done by Bryce Elizabeth Yahn and Lee Aspitarte in 2021.  Built upon work by Raihan Ahmed and Lee Aspitarte in 2020.<br><br>
      </p>

      <button type="button" class="collapsible"> Bios </button>
      <div class="extended">
        <p>
          <span class="bolded"> Lee Aspitarte </span> received his Bachelor of Science in physics from Washington State University, Washington, U.S., in 2011 and his 
          PhD in physics from Oregon State University, Oregon, U.S., in 2017. From July to October of 2017, he was a Postdoctoral Researcher with the National Energy Technology 
          Laboratory. Since 2017, he has been a Principal Research Scientist with Battelle, under the U.S. Department of Energy, at the National Energy Technology Laboratory
           in Albany, Oregon. His interests include experimental research in the field of magnetohydrodynamic power generation and its associated subfields. 
          Dr. Aspitarte’s awards and honors include the Ben and Elaine Whiteley Materials Research Fellowship (Oregon State University) and the Peter Fontana Outstanding Graduate 
          Teaching Assistant Award (Oregon State Department of Physics).<br><br>

          <span class="bolded"> Bryce Elizabeth Yahn </span> Bryce is a recent graduate of University of Rochester where she earned a bachelor of science in brain and cognitive sciences 
          with a minor in mathematics.  She has participated in four research programs on her journey from studying the brain to working in the field of data science and artificial intelligence.  
          She has conducted neuroscience research at the MIT Summer Research program, studied eye movements and attention in Berlin at Humboldt Universitat, and completed machine vision research 
          at University of Rochester.  She is now gaining experience with natural language processing and data visualization working with the Department of Energy to better understand the vast 
          body of scientific literature relating to renewable energy storage.  In September, she will be moving to Arlington, VA to start a data science position working in Leidos’s AI and 
          machine learning accelerator group.  She is passionate about making STEM an inclusive and welcoming field for everyone and hopes to continue working towards that cause in every position 
          she takes.  Outside of work, she is an avid rock climber, and likes to push herself to new heights by facing her fears and challenging herself every day.  She is also interested in 
          photography.  She won a scholarship for a tuition free fifth year of college to study how photography can shape our perspectives and world views.<br><br>
          
          <span class="bolded"> Raihan Ahmed </span> was a rising senior pursuing his Bachelor of Science in computer science and minor in linguistics at Northeastern Illinois 
          University in Chicago, IL at the time of this work. Raihan intends to pursue a PhD in computer science or linguistics. From 2017 to 2018, he was an undergraduate 
          research assistant at the University of Illinois at Urbana-Champaign’s (UIUC) College of Agricultural, Consumer, and Environmental Sciences, as well as a teaching 
          assistant and academic tutor with UIUC’s Department of Physics. Between 2018 and 2019, he was an information systems and technology co-op student with Apple. 
          Since then, he has been a research fellow at the National Energy Technology Laboratory in Albany, Oregon. Mr. Ahmed’s awards and honors include the Mickey Leland 
          Energy Fellowship (Oak Ridge Institute for Science and Education).<br><br>
        </p>
      </div>
    </section>
    <script>
      var coll = document.getElementsByClassName("collapsible");
      var i;
      
      for (i = 0; i < coll.length; i++) {
        coll[i].addEventListener("click", function() {
          this.classList.toggle("active");
          var content = this.nextElementSibling;
          if (content.style.display === "block") {
            content.style.display = "none";
            console.log('hide');
          } else {
            content.style.display = "block";
            console.log('display');
          }
        });
      }
    </script>
  </body>
</html>
