<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>MLEF Energy Storage Visuals</title>
    <link rel="stylesheet" href="main.css">
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500&display=swap" rel="stylesheet">
    <script>
      function expandText(){
        var coll = document.getElementsByClassName("collapsible");
        var i;
        console.log('script called');
        for (i = 0; i < coll.length; i++) {
          coll[i].addEventListener("click", function() {
            this.classList.toggle("active");
            var content = this.nextElementSibling;
            if (content.style.display === "block") {
              console.log('hide');
              content.style.display = "none";
            } else {
              content.style.display = "block";
              console.log('display');
            }
          });
        }
      }
    </script>
  </head>
  <body>
    <header>
      <h1>
        Energy Storage Topic Modeling <br>
      </h1>
    </header>
    <section class="proj">
      <h2> The Problem </h2>
      <p>
        Storing energy from intermittent renewables, such as wind and solar, is one of the most pressing challenges we face for enabling a sustainable civilization. 
        The scale of the problem is immense. 
        A wide array of energy storage technologies are under development, each with their own advantages and disadvantages for various use cases. 
        Scientific research into energy storage technologies has exploded in recent years and sorting through this large body of knowledge to understand the state of field is an important and challenging problem.<br><br>      
      </p>
        
        <button type="button" class="collapsible" onclick="expandText()"> Read More </button>
        <div class="extended">
          <p> Here is more information about the problem </p>
        </div>
      
      <h2> The Goal </h2> 
      <p>
        This project uses artificial intelligence to extract insights from the scientific literature with the goal of being able to better direct research efforts and investments towards promising technologies.
        Below are links to different visualizations of the data using a variety of natural language processing techniques and accompanying visuals.<br><br>
      </p>
      
      <button type="button" class="collapsible"> Read More </button>
      <div class="extended">
        <p> Here is more information about the problem </p>
      </div>
      
      
      <h2> The Data </h2>
      <p>
        Each of the machine learning algorithms used below were trained on a collection of article abstracts pulled from Microsoft Academic related to energy storage.
        The abstracts were obtained with the search term "Energy Storage", keeping the top 100,000 results.
        Duplicate papers were removed (identified by DOI) and opnly articles in english were retained, resulting in approximately 40,000 abstracts.<br><br>
      </p>
      
      <button type="button" class="collapsible"> Read More </button>
      <div class="extended">
        <p> Here is more information about the problem </p>
      </div>
      
      <h2>
        To see our code, please visit our    
        <a class="gitlink" href="https://github.com/MLEF-Energy-Storage"> GitHub </a>
      </h2>
    </section>
    
    <section id="LDA">
      <h2>
        Topic Modeling with LDA and Louvian Communities
      </h2>
      <div class="photo">
        <img src="lda.PNG" alt="lda" class="picture" style='height: 100%; width: 100%; object-fit: contain'>
      </div>
      <div class="description">
        <p>
          <span class="fieldname"> Description: </span> 
          
          Topic modeling is a machine learning problem where you have a collection (corpus) of documents, and you want to extract topics from the unstructured text data.
          In our case, our corpus is the set of abstracts pulled from Microsoft Academic, and we want to extract topics of interesting research from these texts.
          Once we've extracted these topics, we create a graph of topics where each node is a topic and each edge represents the probability of these topics co-occuring in an abstract.
          Different features of the graph convey further information about how prevelant a topic is or how frequently it has shown up in abstracts in the past five years.<br><br>

          <span class="fieldname"> Goal: </span> 
          
          The goal of this modeling and visualization is to algorithmically produce a high-level overview of the field and 
          to allow users to interactively explore different topics and the connections between them.
          We hope this provides insight into surprising connections between different areas of the field or trending topics of interest.
          This visualization can also be used to explore literature in the field related to each topic or their connections.<br><br>
          
          <span class="fieldname"> Algorithms: </span> 
          
        <p>
          This visualization relies primarily on two techniques: Latend Dirichlet Allocation and Louvain Community Detection.<br><br>
     
          <span class="bolded"> Latent Dirichlet Allocation (LDA) </span> is a popular algorithm used for topic modeling.
          It models topics as probability distributions over words.  
          (<span class="emp"> ex: the topic of "pets" contains the word "cat" with a high probability and "solar_panel" with a low probability. </span>)
          Topics can be represented by the words that have the highest probability of belonging to the topic.
          Furthermore, each document has a certain probability of containing each topic.  
          (<span class="emp"> ex: An article about healthy pet diets has a high probability of containing the topics "food", "pets", and "health" and a low probability of containing the topic "foreign_politics."</span>)
          We performed topic modeling with 100 topics and then produced a graph where nodes represent topics and edges represent the probability of topics occuring in the same document.
          (<span class="emp"> ex: the topics "health" and "food" are more likely to co-occur than "pets" and "foreign_politics".</span>) <br><br>
        </p>
          
        <button type="button" class="collapsible"> Read More </button>
        <div class="extended">
          <p> Here is more information about the problem </p>
        </div>

        <p>  
          <span class="bolded"> Louvain Community Detection </span>
          The Louvain Community Detection algorithm is used on large intereconnected graphs to separate nodes into related groups or communities.
          It starts out by defining each node as an individual community.
          It then uses a similarity metric to compare adjacent nodes, and if they are similar enough, it merges the two communities.
          When it has gone through every node, it creates a new graph in which each community is a single node.
          It repeats this process until no nodes are similar enough to merge.
          The end result is a grouping of similar nodes into research communities.
          These are represented by the colors of the nodes in our graph.<br> <br>
        </p>
        
        <button type="button" class="collapsible"> Read More </button>
        <div class="extended">
          <p> Here is more information about the problem </p>
        </div>

        <p>
          <span class="fieldname"> Visuals: </span> 
          <a class="btn" href="ES_networkplot_7_13.html"> Energy Storage Network Plot </a>
        </p>
        
      </div>
    </section>
    <section id="w2v">
      <h2>
        Word2Vec Visualized with PCA
      </h2>
      <div class="photo">
        <img src="w2v.png" alt="w2v" class="picture" style='height: 100%; width: 100%; object-fit: contain'>
      </div>
      <div class="description">
        <p>
          <span class="fieldname"> Description: </span> 
          In natural language processing, words are often embedded into a numerical vector that encodes the meaning of the words.
          Similarly to how we embed colors by their RGB values, we can encode words using numerical vectors.
          Unlike with colors where the first dimension represents the red component etc, we don't know what each dimension in a word vector represents.
          Instead, the embeddings are learned from the context and surrounding words and machine learning algorithms.
          These word vectors can be conceptualized as a point in high dimensional space.
          We can easily compare how close these vectors are in high dimensional space, giving us a good idea of how similar the words are.
          We then projected these words onto a two dimensional plane so that we could spacially visualize the relationships between different words.
          Another cool feature of word vectors is that you can do math with them.  
          By adding and subtracting vectors, you can search for new, relevant words.
          <span class="emp"> ex: if you take the embedding of "King" and subtract the embedding of "Man", the resultant vector represents the relationship between "King" and "Man".
          If you then add this new vector to the embedding of "Woman", the resulting vector is most similar to the embedding of "Queen".</span><br><br>
        </p>
        <p>
          <span class="fieldname"> Goal: </span> 
          The goal of this visualization is to be able to choose words out of the collection of abstracts and explore their relationships either by
          seeing how close they are on the two dimensional visualization, or my experimenting with vector math. <br><br>
        </p>
        
        <p>
          <span class="fieldname"> Algorithms: </span> 
          
          This visualization relies primarily on two techniques: Word2Vec and Principal Component Analysis.<br><br>
          
          <span class="bolded"> Word2Vec </span>
          Word2vec is a word embedding technique that uses a neural network to learn word associations from a large corpus of text.
          For each word, it uses the current word embedding to try to predict the words in some window around it.
          When it incorrectly predicts a word, it uses the error to update the word embedding and improve.
          The end result is a word embedding for each word in the corpus that is learned from the context of surrounding words.
          <br><br>
        </p>
        
        <button type="button" class="collapsible"> Read More </button>
        <div class="extended">
          <p> Here is more information about the problem </p>
        </div>
        
        <p>
          <span class="bolded"> Principal Component Analysis (PCA) </span>
          is a dimensionality reduction technique that projects points in higher dimensions into a lower dimensional space.
          Each of the word vectors are 200 dimensional, but we can visualize at most 3 dimensions, and in this case, we want to view the vectors on a 2D screen.
          There are many different ways you could project down to 2D, but we want to choose the one that preserves as much of the initial information as possible.
          PCA uses linear algebra to find the projection that preserves the most variance/relevant information so that we can visualize these 200D vectors on a 2D screen without losing too much information.
          <br><br>
        </p>
        
        <button type="button" class="collapsible"> Read More </button>
        <div class="extended">
          <p> Here is more information about the problem </p>
        </div>
        
        <p>
          <span class="fieldname"> Visuals: </span> 
          <a class="btn" href="W2V_PCA.html"> Word2Vector Plot </a>
        </p>
      </div>
    </section>
  </body>
</html>
